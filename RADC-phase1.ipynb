{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we have two kind of datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1- data_long has all the test results we needed for the fu_years categories like:\n",
    "\n",
    "        blood measures\n",
    "        cognitions\n",
    "        Progressive metrics used for cognitions\n",
    "        demographics\n",
    "        Disabilites\n",
    "        lifestyle\n",
    "        Medical conditions-(cumulative of them)\n",
    "        Medications currently going on\n",
    "        \n",
    "2- data_basic has all informations for:\n",
    "\n",
    "        brain protiens \n",
    "        age_death\n",
    "        race,sex,educ, \n",
    "        baselines for all medical conditions \n",
    "        pathology test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_long = pd.read_excel(\"/Users/charusaxena/Dropbox/RADC dataset/dataset_576_long.xlsx\")\n",
    "data_basic = pd.read_excel(\"/Users/charusaxena/Dropbox/RADC dataset/dataset_576_basic.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for df (25570, 203)\n",
      "shape for df2 (3311, 49)\n"
     ]
    }
   ],
   "source": [
    "df = data_long.copy()\n",
    "df2 = data_basic.copy()\n",
    "df.head()\n",
    "df2.head()\n",
    "\n",
    "print(\"shape for df\",df.shape)\n",
    "print(\"shape for df2\",df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets start with the Exploratory Data analysis for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df1.isnull().sum())\n",
    "print(df2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset long\n",
    "total_cells= np.product(df.shape)\n",
    "missing_data = df.isnull().sum()\n",
    "missing = missing_data.sum()\n",
    "percentage = (missing/total_cells)*100\n",
    "\n",
    "#for dataset basic\n",
    "total_cells1= np.product(df2.shape)\n",
    "missing_data1 = df2.isnull().sum()\n",
    "missing1 = missing_data1.sum()\n",
    "percentage1 = (missing1/total_cells1)*100\n",
    "\n",
    "print(\"the total missing percentage data in data_long is: \",percentage)\n",
    "print(\"the total missing percentage data in data_basic is: \",percentage1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that we have only 15% of missing data from long whereas we have nearly half of the dataset empty in case of basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As suggested by professors am goning to keep important features only in data_long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"anemia_who\"]\n",
    "del df[\"bun\"]\n",
    "del df[\"cl\"]\n",
    "del df[\"co2\"]\n",
    "del df[\"fasting\"]\n",
    "del df[\"k\"]\n",
    "del df[\"na\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"rdw\"]\n",
    "del df[\"tga\"]\n",
    "del df[\"tsh\"]\n",
    "del df[\"wbc\"]\n",
    "\n",
    "del df[\"confid_health\"]\n",
    "del df[\"confid_instit\"]\n",
    "del df[\"finfintot\"]\n",
    "del df[\"finhlthtot\"]\n",
    "del df[\"finuctot\"]\n",
    "del df[\"fraud7\"]\n",
    "del df[\"scam\"]\n",
    "del df[\"gamma\"]\n",
    "del df[\"risk\"]\n",
    "del df[\"large_alpha\"]\n",
    "del df[\"small_alpha\"]\n",
    "del df[\"fin_literacy_pct\"]\n",
    "del df[\"health_literacy_pct\"]\n",
    "del df[\"literacy_total_pct\"]\n",
    "\n",
    "del df[\"thyroid_cum\"]\n",
    "del df[\"chf_cum\"]\n",
    "del df[\"heart_cum\"]\n",
    "\n",
    "del df[\"antibiotic_rx\"]\n",
    "del df[\"antihyp_all_rx\"]\n",
    "del df[\"antineoplastic_rx\"]\n",
    "del df[\"cardiac_rx\"]\n",
    "del df[\"dental_rx\"]\n",
    "del df[\"dermatologic_rx\"]\n",
    "del df[\"endocrine_rx\"]\n",
    "del df[\"gastrointestinal_rx\"]\n",
    "del df[\"hemotologic_rx\"]\n",
    "del df[\"lipid_lowering_rx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df[\"musculoskeletal_rx\"]\n",
    "del df[\"ophthalmic_rx\"]\n",
    "del df[\"otic_rx\"]\n",
    "del df[\"respiratory_rx\"]\n",
    "del df[\"supplement_rx\"]\n",
    "del df[\"urinary_rx\"]\n",
    "del df[\"acetaminophen_rx\"]\n",
    "del df[\"antiinfective_rx\"]\n",
    "del df[\"vaccine_rx\"]\n",
    "del df[\"antiarrhythmic_rx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df[\"diabetes_rx\"]\n",
    "del df[\"antacid_rx\"]\n",
    "del df[\"antidiarrheal_rx\"]\n",
    "del df[\"antinausea_rx\"]\n",
    "del df[\"antireflux_rx\"]\n",
    "del df[\"laxative_rx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df[\"anticoagulant_rx\"]\n",
    "del df[\"osteoporosis_rx\"]\n",
    "del df[\"antiasthmatic_rx\"]\n",
    "del df[\"antihistamine_rx\"]\n",
    "del df[\"coughcoldallergy_rx\"]\n",
    "del df[\"nasal_rx\"]\n",
    "del df[\"alternative_rx\"]\n",
    "del df[\"cartilage_base_rx\"]\n",
    "del df[\"fish_oil_supplement_rx\"]\n",
    "del df[\"glucosamine_rx\"]\n",
    "del df[\"herbals_rx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df[\"macronutrient_rx\"]\n",
    "del df[\"micronutrient_rx\"]\n",
    "del df[\"mineral_rx\"]\n",
    "del df[\"multivitamin_rx\"]\n",
    "del df[\"nutrient_rx\"]\n",
    "del df[\"other_dietary_rx\"]\n",
    "del df[\"vitamin_minerals_rx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df[\"vitamin_rx\"]\n",
    "del df[\"alphablocker_rx\"]\n",
    "del df[\"benign_pros_hyper_rx\"]\n",
    "del df[\"bphmed_rx\"]\n",
    "del df[\"estrogen_vaginal_rx\"]\n",
    "del df[\"urinary_antispas_rx\"]\n",
    "del df[\"urinary_inc_rx\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As suggested by professors am goning to keep important features only in data_basic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df2[\"apoe_genotype\"]\n",
    "del df2[\"educ\"]\n",
    "#del df2[\"msex\"]\n",
    "del df2[\"race\"]\n",
    "del df2[\"spanish\"]\n",
    "del df2[\"thyroid_bl\"]\n",
    "del df2[\"chf_bl\"]\n",
    "del df2[\"pmi\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the reamining number of features and if there is null values in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset long\n",
    "total_cells= np.product(df.shape)\n",
    "missing_data = df.isnull().sum()\n",
    "missing = missing_data.sum()\n",
    "percentage = (missing/total_cells)*100\n",
    "\n",
    "#for dataset basic\n",
    "total_cells1= np.product(df2.shape)\n",
    "missing_data1 = df2.isnull().sum()\n",
    "missing1 = missing_data1.sum()\n",
    "percentage1 = (missing1/total_cells1)*100\n",
    "\n",
    "print(\"the total missing percentage data in data_long is: \",percentage)\n",
    "print(\"the total missing percentage data in data_basic is: \",percentage1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing all people with Cancer and Diabetes and with poeple with head injuries:\n",
    "    so will be removing paitents who happen to have medical conditions of these two or \n",
    "    taking medications prescribed for these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how many unique encodinga are there before removing people:\n",
    "df.headinjrloc_cum.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing people who have a suspected of having cancer in past visits \n",
    "df = df[df.cancer_cum !=1]\n",
    "df2 = df2[df2.cancer_bl !=1]\n",
    "\n",
    "#removing people who have a suspected of having diabetes in past visits \n",
    "df = df[df.dm_cum !=1]\n",
    "df2 = df2[df2.diabetes_sr_rx_bl !=1]\n",
    "\n",
    "#removing people who have a suspected of having head injuries in past visits \n",
    "df = df[df.headinjrloc_cum !=1]\n",
    "df2 = df2[df2.headinjrloc_bl !=1]\n",
    "\n",
    "#medications with analgesic_rx\n",
    "df = df[df.analgesic_rx !=1]\n",
    "\n",
    "#medications with insulin\n",
    "#?? still in doubt \n",
    "#df1 = df1[df1.diabetes_insulin_rx !=1]\n",
    "#df1 = df1[df1.diabetes_noninsulin_rx !=1]\n",
    "#df1 = df1[df1.diabetes_rx!=1]\n",
    "\n",
    "#here it indicates any sort of medical history 1-yes , 0-No\n",
    "#df2 = df2[df2.diabetes_sr_rx_bl !=1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before mixing bith the daatsets in reference to thier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.projid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.projid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding a column such that fu_year are seperated as 0 and rest\n",
    "df['test'] = df['fu_year'].map(lambda x : 'year' if x>0 else 'baseline')\n",
    "\n",
    "#converting study to integer from string(map-1 , ros=0)\n",
    "df['study'] = df['study'].map(lambda x : 1 if x=='MAP' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with baseline year\n",
    "df_with_baseline_year = df.loc[df['test'] == 'baseline']\n",
    "#df.loc[df['fu_year'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making different dataframes for all years seperately\n",
    "df_with_1_year = df.loc[df['fu_year'] == 1]\n",
    "df_with_2_year = df.loc[df['fu_year'] == 2]\n",
    "df_with_3_year = df.loc[df['fu_year'] == 3]\n",
    "df_with_4_year = df.loc[df['fu_year'] == 4]\n",
    "df_with_5_year = df.loc[df['fu_year'] == 5]\n",
    "df_with_6_year = df.loc[df['fu_year'] == 6]\n",
    "df_with_7_year = df.loc[df['fu_year'] == 7]\n",
    "df_with_8_year = df.loc[df['fu_year'] == 8]\n",
    "df_with_9_year = df.loc[df['fu_year'] == 9]\n",
    "df_with_10_year = df.loc[df['fu_year'] == 10]\n",
    "df_with_11_year = df.loc[df['fu_year'] == 11]\n",
    "df_with_12_year = df.loc[df['fu_year'] == 12]\n",
    "df_with_13_year = df.loc[df['fu_year'] == 13]\n",
    "df_with_14_year = df.loc[df['fu_year'] == 14]\n",
    "df_with_15_year = df.loc[df['fu_year'] == 15]\n",
    "df_with_16_year = df.loc[df['fu_year'] == 16]\n",
    "df_with_17_year = df.loc[df['fu_year'] == 17]\n",
    "df_with_18_year = df.loc[df['fu_year'] == 18]\n",
    "df_with_19_year = df.loc[df['fu_year'] == 19]\n",
    "df_with_20_year = df.loc[df['fu_year'] == 20]\n",
    "df_with_21_year = df.loc[df['fu_year'] == 21]\n",
    "df_with_22_year = df.loc[df['fu_year'] == 22]\n",
    "df_with_23_year = df.loc[df['fu_year'] == 23]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first considering just the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#removing all the infinite values before split\n",
    "df_with_baseline_year = df_with_baseline_year[np.isfinite(df_with_baseline_year['dcfdx'])]\n",
    "\n",
    "df_with_baseline_year.replace([np.inf, -np.inf], np.nan)\n",
    "df_with_baseline_year= df_with_baseline_year.fillna(df_with_baseline_year.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_baseline_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train = df_with_baseline_year['dcfdx']\n",
    "x_train = df_with_baseline_year\n",
    "del x_train['dcfdx']\n",
    "del x_train['test']\n",
    "del x_train['scaled_to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del x_train['projid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, y_test = train_test_split( x_train, y_train, test_size=0.33, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#treating nan values\n",
    "#import numpy as np\n",
    "#from sklearn.preprocessing import Imputer\n",
    "#imputer = Imputer()\n",
    "#transformed_X = imputer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##np.any(np.isnan(y_train))# prooving even traget class has missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic REgression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l1 = LogisticRegression(penalty='l1')\n",
    "l2 = LogisticRegression(penalty='l2')\n",
    "model1 = l1.fit(X_train,Y_train)\n",
    "model2 = l2.fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print top 10 features with weigjts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_features_wgt=np.argsort(np.abs(l1.coef_[0]))[::-1]\n",
    "flag=10\n",
    "print(\"top 10 features:\\n\")\n",
    "for i in l1_features_wgt:\n",
    "    if flag>0:\n",
    "        print(\"%s\\t%0.3f\" %(df.columns[i], l1.coef_[0][i]))\n",
    "        flag-=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l2 with top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_features_wgt=np.argsort(np.abs(l2.coef_[0]))[::-1]\n",
    "flag=10\n",
    "print(\"top 10 features:\\n\")\n",
    "for i in l2_features_wgt:\n",
    "    if flag>0:\n",
    "        print(\"%s\\t%0.3f\" %(df.columns[i], l2.coef_[0][i]))\n",
    "        flag-=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Total_evidence(object):\n",
    "    #instnce\n",
    "    positive_evidence=[]\n",
    "    negative_evidence=[]\n",
    "    evidence = []\n",
    "    for i,val in enumerate(object):\n",
    "        evidence.append(val* l2.coef_[0][i])\n",
    "        if val* l2.coef_[0][i] >0:\n",
    "            positive_evidence.append(val* l2.coef_[0][i])\n",
    "        else:\n",
    "            negative_evidence.append(val* l2.coef_[0][i])\n",
    "            \n",
    "    total_positive_evidence=sum(positive_evidence)\n",
    "    total_negative_evidence=sum(negative_evidence)\n",
    "    return total_positive_evidence, total_negative_evidence,evidence\n",
    "\n",
    "def evidence(object):\n",
    "    #instnce\n",
    "    positive_evidence=[]\n",
    "    negative_evidence=[]\n",
    "    \n",
    "    for i,val in enumerate(object):\n",
    "        if val* l2.coef_[0][i] >0:\n",
    "            positive_evidence.append(val* l2.coef_[0][i])\n",
    "        else:\n",
    "            negative_evidence.append(val* l2.coef_[0][i])\n",
    "    return positive_evidence, negative_evidence\n",
    "\n",
    "def top_features(evidence,pos,neg):\n",
    "    \n",
    "    Top_neg_index = np.argsort(evidence)\n",
    "    \n",
    "    #Top_pos_index = evidence.argsort()[::-1][:n]\n",
    "    if neg>3:\n",
    "        idx = 3\n",
    "    else:\n",
    "        idx=neg\n",
    "        \n",
    "    if pos>3:\n",
    "        idx1 = 3\n",
    "    else:\n",
    "        idx1=pos  \n",
    "    return Top_neg_index[:idx],Top_neg_index[-idx1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = l2.predict_proba(X_test)\n",
    "np.argmax(predict[:,1])\n",
    "X_test.iloc[np.argmax(predict[:,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = l2.predict_proba(X_test)\n",
    "print(predict)\n",
    "\n",
    "most_postive_object = X_test.iloc[(np.argmax(predict[:,1]))]\n",
    "print(\"most positive object wrt prob:\\n\",most_postive_object )\n",
    "\n",
    "a = Total_evidence(most_postive_object)\n",
    "print(\"\\ntotal positive evidence:\",a[0])\n",
    "print(\"\\ntotal negative evidence:\",a[1])\n",
    "print(\"\\nclass distributuion\",(predict[np.argmax(predict[:,1])]))\n",
    "print(\"\\n evidences: \",a[2])\n",
    "\n",
    "#for top features:\n",
    "pos_evi=evidence(most_postive_object)[0]\n",
    "neg_evi=evidence(most_postive_object)[1]\n",
    "\n",
    "indexNeg= top_features(a[2],len(pos_evi),len(neg_evi))[0]\n",
    "indexPos = top_features(a[2],len(pos_evi),len(neg_evi))[1]\n",
    "#print(indexNeg,indexPos)\n",
    "columns = X_test.columns\n",
    "\n",
    "print(\"\\n\\ntop negative features are:\",columns[indexNeg],\"\\ntop positive features\",columns[indexPos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted = cross_val_predict(model1, X_test, y_test, cv=10)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "model3=clf.fit(X_train.values, Y_train.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.support_vectors_\n",
    "model3.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x=np.array(X_train.astype(np.integer))\n",
    "#y=np.array(Y_train.astype(np.integer))\n",
    "xarray = X_train.as_matrix\n",
    "df_to_nparray = X_train.to_records(index=False)\n",
    "\n",
    "df_to_nparray.astype(float)\n",
    "df_to_nparray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plot_decision_regions(X=df_to_nparray , y=y, clf=clf, res=0.02,legend=2)\n",
    "\n",
    "# Update plot object with X/Y axis labels and Figure Title\n",
    "plt.xlabel(X.columns[0], size=14)\n",
    "plt.ylabel(X.columns[1], size=14)\n",
    "plt.title('SVM Decision Region Boundary', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
